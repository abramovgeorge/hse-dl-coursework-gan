import warnings
from sys import argv

import numpy as np
import pandas as pd
from catboost import CatBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.exceptions import ConvergenceWarning
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
)
from tqdm import tqdm

warnings.filterwarnings("ignore", category=ConvergenceWarning)


def main(dataset_name, model_name, path=""):
    """
    Calculate metrics of classifiers on the dataset generated by a certain model trained on a certain dataset.
    Args:
        dataset_name (str): name of the dataset
        model_name (str): name of the model
        path (str): path to the datasets folder
    """

    targets = {"adult": "income", "credit": "Class"}

    if len(path) > 0 and path[-1] != "/":
        path += "/"

    test = pd.read_csv(f"{path}{dataset_name}/test.csv")

    train = pd.read_csv(f"{path}{dataset_name}/{model_name}.csv")
    train = pd.DataFrame(data=train.values, columns=test.columns)

    target = targets[dataset_name]

    seeds = list(range(5))

    for column in test.columns[:-1]:
        if test.dtypes[column] == np.int64:
            train[column] = round(train[column])

    if train[target].unique().shape[0] != 2:
        train[target] = train[target] >= train[target].median()

    scores = {
        "cb": {"f1": [], "roc_auc": [], "acc": [], "rec": [], "pre": []},
        "rf": {"f1": [], "roc_auc": [], "acc": [], "rec": [], "pre": []},
        "lr": {"f1": [], "roc_auc": [], "acc": [], "rec": [], "pre": []},
    }

    for seed in tqdm(seeds):
        models = dict()
        models["cb"] = CatBoostClassifier(random_state=seed, verbose=0)
        models["rf"] = RandomForestClassifier(random_state=seed, n_jobs=-1)
        models["lr"] = LogisticRegression(random_state=seed)
        for name, model in models.items():
            model.fit(train.drop(columns=[target]), train[target])
            preds = model.predict(test.drop(columns=[target]))
            scores[name]["f1"].append(f1_score(test[target], preds))
            scores[name]["roc_auc"].append(roc_auc_score(test[target], preds))
            scores[name]["acc"].append(accuracy_score(test[target], preds))
            scores[name]["rec"].append(recall_score(test[target], preds))
            scores[name]["pre"].append(precision_score(test[target], preds))

    print(f"\n{dataset_name}/{model_name}:\n")
    for name in scores.keys():
        f1 = scores[name]["f1"]
        auc = scores[name]["roc_auc"]
        acc = scores[name]["acc"]
        rec = scores[name]["rec"]
        pre = scores[name]["pre"]
        print(
            f"scores for {name}:\n"
            f"f1: mean, std: {np.around(np.mean(f1), decimals=3)}, {np.around(np.std(f1), decimals=3)}\n"
            f"roc_auc: mean, std: {np.around(np.mean(auc), decimals=3)}, {np.around(np.std(auc), decimals=3)}\n"
            f"accuracy: mean, std: {np.around(np.mean(acc), decimals=3)}, {np.around(np.std(acc), decimals=3)}\n"
            f"recall: mean, std: {np.around(np.mean(rec), decimals=3)}, {np.around(np.std(rec), decimals=3)}\n"
            f"precision: mean, std: {np.around(np.mean(pre), decimals=3)}, {np.around(np.std(pre), decimals=3)}\n"
        )


if __name__ == "__main__":
    dataset_name = argv[1]
    model_name = argv[2]
    path = ""
    if len(argv) > 3:
        path = argv[3]
    main(dataset_name, model_name, path)
